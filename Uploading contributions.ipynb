{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nebula/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/matplotlib/__init__.py:1350: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got full_df\n",
      "got parsed_df\n",
      "got full_df\n",
      "got parsed_df\n"
     ]
    }
   ],
   "source": [
    "# do basic imports and unpack McMurdo data\n",
    "\n",
    "#from pmagpy import ipmag\n",
    "#reload(ipmag)\n",
    "from pmagpy import pmag\n",
    "from programs import new_builder as nb\n",
    "from programs import data_model3\n",
    "reload(data_model3)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from programs.new_builder import Contribution\n",
    "\n",
    "import pmagpy.controlled_vocabularies3 as cv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-W- No such file: /Users/nebula/Python/PmagPy/3_0/Megiddo/images.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_high</th>\n",
       "      <th>age_low</th>\n",
       "      <th>age_unit</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>method_codes</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-740</td>\n",
       "      <td>-732</td>\n",
       "      <td>-800</td>\n",
       "      <td>Years Cal AD (+/-)</td>\n",
       "      <td>\"Tel-Hazor chronology. 2015 revision. Amnon Be...</td>\n",
       "      <td>Tel Hazor</td>\n",
       "      <td>GM-C14:GM-CC-ARCH</td>\n",
       "      <td>hz05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a string</td>\n",
       "      <td>-732</td>\n",
       "      <td>1e+12</td>\n",
       "      <td>Years Cal AD (+/-)</td>\n",
       "      <td>\"Tel-Hazor chronology. 2015 revision. Amnon Be...</td>\n",
       "      <td>Tel Hazor</td>\n",
       "      <td>GM-C14:GM-CC-ARCH</td>\n",
       "      <td>fake site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-850</td>\n",
       "      <td>-800</td>\n",
       "      <td>-900</td>\n",
       "      <td>Years Cal AD (+/-)</td>\n",
       "      <td>\"Tel-Hazor chronology. 2015 revision. Amnon Be...</td>\n",
       "      <td>Tel Hazor</td>\n",
       "      <td>GM-C14:GM-CC-ARCH</td>\n",
       "      <td>hz07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-950</td>\n",
       "      <td>-900</td>\n",
       "      <td>-1000</td>\n",
       "      <td>Years Cal AD (+/-)</td>\n",
       "      <td>\"Tel-Hazor chronology. 2015 revision. Amnon Be...</td>\n",
       "      <td>Tel Hazor</td>\n",
       "      <td>GM-C14:GM-CC-ARCH</td>\n",
       "      <td>hz09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-950</td>\n",
       "      <td>-900</td>\n",
       "      <td>-1000</td>\n",
       "      <td>Years Cal AD (+/-)</td>\n",
       "      <td>\"Tel-Hazor chronology. 2015 revision. Amnon Be...</td>\n",
       "      <td>Tel Hazor</td>\n",
       "      <td>GM-C14:GM-CC-ARCH</td>\n",
       "      <td>hz10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age age_high age_low            age_unit  \\\n",
       "0      -740     -732    -800  Years Cal AD (+/-)   \n",
       "1  a string     -732   1e+12  Years Cal AD (+/-)   \n",
       "2      -850     -800    -900  Years Cal AD (+/-)   \n",
       "3      -950     -900   -1000  Years Cal AD (+/-)   \n",
       "4      -950     -900   -1000  Years Cal AD (+/-)   \n",
       "\n",
       "                                         description   location  \\\n",
       "0  \"Tel-Hazor chronology. 2015 revision. Amnon Be...  Tel Hazor   \n",
       "1  \"Tel-Hazor chronology. 2015 revision. Amnon Be...  Tel Hazor   \n",
       "2  \"Tel-Hazor chronology. 2015 revision. Amnon Be...  Tel Hazor   \n",
       "3  \"Tel-Hazor chronology. 2015 revision. Amnon Be...  Tel Hazor   \n",
       "4  \"Tel-Hazor chronology. 2015 revision. Amnon Be...  Tel Hazor   \n",
       "\n",
       "        method_codes       site  \n",
       "0  GM-C14:GM-CC-ARCH       hz05  \n",
       "1  GM-C14:GM-CC-ARCH  fake site  \n",
       "2  GM-C14:GM-CC-ARCH       hz07  \n",
       "3  GM-C14:GM-CC-ARCH       hz09  \n",
       "4  GM-C14:GM-CC-ARCH       hz10  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = os.path.join(os.getcwd(), '3_0', 'Megiddo')\n",
    "con = Contribution(dir_path)\n",
    "\n",
    "loc_dm = con.tables['locations'].data_model.dm['locations']\n",
    "loc_df = con.tables['locations'].df\n",
    "site_dm = con.tables['sites'].data_model.dm['sites']\n",
    "site_df = con.tables['sites'].df\n",
    "samp_df = con.tables['samples'].df\n",
    "samp_dm = con.tables['samples'].data_model.dm['samples']\n",
    "spec_df = con.tables['specimens'].df\n",
    "spec_dm = con.tables['specimens'].data_model.dm['specimens']\n",
    "age_df = con.tables['ages'].df\n",
    "age_dm = con.tables['ages'].data_model.dm['ages']\n",
    "meas_df = con.tables['measurements'].df\n",
    "meas_dm = con.tables['measurements'].data_model.dm['measurements']\n",
    "cont_df = con.tables['contribution'].df\n",
    "cont_dm = con.tables['contribution'].data_model.dm['contribution']\n",
    "crit_df = con.tables['criteria'].df\n",
    "crit_dm = con.tables['criteria'].data_model.dm['criteria']\n",
    "\n",
    "\n",
    "current_con = con\n",
    "\n",
    "# mess up some validations for locations\n",
    "loc_df.loc['Tel Hazor', 'lat_s'] = 400.\n",
    "loc_df['dir_inc'] = 5\n",
    "loc_df.loc['Tel Hazor', 'lat_n'] = 'hello'\n",
    "loc_df.loc[:, 'lithologies'] = [\"Agate:Basalt\", \"Basalt:random\"]\n",
    "#current_con.tables.pop('sites')\n",
    "\n",
    "# mess up some validations for sites\n",
    "site_df.pop('age')\n",
    "site_df['dir_tilt_correction'] = 1\n",
    "site_df['dir_tilt_correction'] = 'one'\n",
    "site_df.iloc[0, list(site_df.columns).index('lithologies')] = \"Angrite:Basalt\"\n",
    "site_df.iloc[1, list(site_df.columns).index('lithologies')] = \"angrite : basalt\"\n",
    "\n",
    "# mess up some validations for ages\n",
    "age_df.ix[1]['age'] = 'a string'\n",
    "age_df.ix[1]['site'] = 'fake site'\n",
    "age_df.ix[1]['age_low'] = 1000000000000.\n",
    "age_df.pop('citations')\n",
    "\n",
    "# mess up some validations for samples\n",
    "samp_df.pop('citations')\n",
    "samp_df.iloc[0].lon = 600.\n",
    "samp_df.iloc[0].age = \"another string\"\n",
    "samp_df.iloc[0].lat = \"stringy\"\n",
    "samp_df.iloc[1].lat = 'hello'\n",
    "samp_df.iloc[2].specimens = \"hz05a2:fake\"\n",
    "samp_df.iloc[3].specimens = \"fake : hz05a1\"\n",
    "samp_df.iloc[5].specimens = 'fake_specimen'\n",
    "samp_df.iloc[7].site = 'fake_site'\n",
    "samp_df.iloc[0].cooling_rate = 'a string'\n",
    "\n",
    "# mess up some validations for measurements\n",
    "meas_df.loc['mgh05a01:LP-PI-TRM1', 'magn_moment'] = 2\n",
    "meas_df.loc['mgh05a01:LP-PI-TRM1', 'specimen'] = \"fake_specimen\"\n",
    "meas_df.pop('experiment')\n",
    "\n",
    "#current_df.head()\n",
    "#current_df.head()\n",
    "age_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Importing controlled vocabularies from https://earthref.org\n"
     ]
    }
   ],
   "source": [
    "import pmagpy.controlled_vocabularies3 as cv\n",
    "reload(cv)\n",
    "vocab = cv.Vocabulary()\n",
    "vocabulary, possible_vocabulary = vocab.get_controlled_vocabularies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## validation functions\n",
    "\n",
    "\n",
    "# need to add requiredOneInGroup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def requiredUnless(col_name, arg, dm, df):\n",
    "    \"\"\"\n",
    "    Arg is a string in the format \"str1, str2, ...\"\n",
    "    Each string will be a column name.\n",
    "    Col_name is required in df unless each column from arg is present.\n",
    "    \"\"\"\n",
    "    arg_list = arg.split(\",\")\n",
    "    arg_list = [arg.strip('\"') for arg in arg_list]\n",
    "    msg = \"\"\n",
    "    for a in arg_list:\n",
    "        # ignore validations that reference a different table\n",
    "        if \".\" in a:\n",
    "            continue\n",
    "        if a not in df.columns:\n",
    "            msg += \"{} column is required unless {} is present.  \".format(col_name, a)\n",
    "    if msg:\n",
    "        return msg\n",
    "    else:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def requiredUnlessTable(col_name, arg, dm, df):\n",
    "    \"\"\"\n",
    "    Col_name must be present in df unless\n",
    "    arg (table_name) is present in contribution\n",
    "    \"\"\"\n",
    "    table_name = arg\n",
    "    if col_name in df.columns:\n",
    "        return None\n",
    "    elif table_name in current_con.tables:\n",
    "        return None\n",
    "    else:\n",
    "        #print \"{} is required unless table {} is present\".format(col_name, table_name)\n",
    "        return \"{} column is required unless table {} is present\".format(col_name, table_name)\n",
    "\n",
    "    \n",
    "def requiredIfGroup(col_name, arg, dm, df):\n",
    "    \"\"\"\n",
    "    Col_name is required if other columns of \n",
    "    the group arg are present.\n",
    "    \"\"\"\n",
    "    group_name = arg\n",
    "    groups = set()\n",
    "    columns = df.columns\n",
    "    for col in columns:\n",
    "        if col not in dm.index:\n",
    "            continue\n",
    "        group = dm.loc[col]['group']\n",
    "        groups.add(group)\n",
    "    if group_name in groups:\n",
    "        if col_name in columns:\n",
    "            return None\n",
    "        else:\n",
    "            #print \"{} is required if column group {} is used\".format(col_name, group_name)\n",
    "            return \"{} column is required if column group {} is used\".format(col_name, group_name)\n",
    "    return None\n",
    "\n",
    "\n",
    "def required(col_name, arg, dm, df):\n",
    "    \"\"\"\n",
    "    Col_name is required in df.columns.\n",
    "    Return error message if not.\n",
    "    \"\"\"\n",
    "    if col_name in df.columns:\n",
    "        return None\n",
    "    else:\n",
    "        return '\"{}\" column is required'.format(col_name) \n",
    "\n",
    "def isIn(row, col_name, arg, dm, df):\n",
    "    \"\"\"\n",
    "    row[col_name] must contain a value from another column.\n",
    "    If not, return error message.\n",
    "    \"\"\"\n",
    "    #grade = df.apply(func, args=(validation_name, arg, dm), axis=1)\n",
    "    x = 0\n",
    "    cell_value = row[col_name]\n",
    "    if not cell_value:\n",
    "        return None\n",
    "    # if it's in another table\n",
    "    cell_values = [v.strip(\" \") for v in cell_value.split(\":\")]\n",
    "    if \".\" in arg:\n",
    "        table_name, table_col_name = arg.split(\".\")\n",
    "        if table_name not in current_con.tables:\n",
    "            return \"Must contain a value from {} table. Missing {} table.\".format(table_name, table_name)\n",
    "        if table_col_name not in current_con.tables[table_name].df.columns:\n",
    "            return '{} table is missing \"{}\" column, which is required for validating \"{}\" column'.format(table_name, table_col_name, col_name)\n",
    "        possible_values = current_con.tables[table_name].df[table_col_name].unique()\n",
    "        for value in cell_values:\n",
    "            if value not in possible_values:\n",
    "                return 'This value: \"{}\" is not found in: {}'.format(value, arg)\n",
    "                break\n",
    "    # if it's in the present table:\n",
    "    else:\n",
    "        possible_values = df[arg].unique()\n",
    "        for value in cell_values:\n",
    "            if value not in possible_values:\n",
    "                return 'This value: \"{}\" is not found in: {} column'.format(value, arg)\n",
    "                break\n",
    "    return None\n",
    "    \n",
    "def checkMax(row, col_name, arg, *args):\n",
    "    \"\"\"\n",
    "    row[col_name] must be less than or equal to arg.\n",
    "    else, return error message.\n",
    "    \"\"\"\n",
    "    cell_value = row[col_name]\n",
    "    if not cell_value:\n",
    "        return None\n",
    "    try:\n",
    "        arg_val = float(arg)\n",
    "    except ValueError:\n",
    "        arg_val = row[arg]\n",
    "    #arg = float(arg)\n",
    "    try:\n",
    "        if float(cell_value) <= float(arg_val):\n",
    "            return None\n",
    "        else:\n",
    "            #print \"{} must be <= {}\".format(str(cell_value), str(arg))\n",
    "            return \"{} ({}) must be <= {} ({})\".format(str(cell_value), col_name, str(arg_val), str(arg))\n",
    "    # this happens when the value isn't a float (an error which will be caught elsewhere)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def checkMin(row, col_name, arg, *args):\n",
    "    \"\"\"\n",
    "    row[col_name] must be greater than or equal to arg.\n",
    "    else, return error message.\n",
    "    \"\"\"\n",
    "    cell_value = row[col_name]\n",
    "    if not cell_value:\n",
    "        return None\n",
    "    try:\n",
    "        arg_val = float(arg)\n",
    "    except ValueError:\n",
    "        arg_val = row[arg]\n",
    "    try:\n",
    "        if float(cell_value) >= float(arg_val):\n",
    "            return None\n",
    "        else:\n",
    "            return \"{} ({}) must be >= {} ({})\".format(str(cell_value), col_name, arg_val, str(arg))\n",
    "    # this happens when the value isn't a float (an error which will be caught elsewhere)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def cv(row, col_name, arg, current_data_model, *args):\n",
    "    \"\"\"\n",
    "    row[col_name] must contain only values from the appropriate controlled vocabulary\n",
    "    \"\"\"\n",
    "    cell_value = row[col_name]\n",
    "    if not cell_value:\n",
    "        return None\n",
    "    cell_values = cell_value.split(\":\")\n",
    "    cell_values = [c.strip() for c in cell_values]\n",
    "    for value in cell_values:\n",
    "        if value.lower() in [v.lower() for v in vocabulary[col_name]]:\n",
    "            continue\n",
    "        else:\n",
    "            return '\"{}\" is not in controlled vocabulary for {}'.format(value, arg)\n",
    "    return None\n",
    "        \n",
    "\n",
    "# validate presence\n",
    "presence_operations = {\"required\": required, \"requiredUnless\": requiredUnless,\n",
    "                       \"requiredIfGroup\": requiredIfGroup, \n",
    "                       'requiredUnlessTable': requiredUnlessTable}\n",
    "# validate values\n",
    "value_operations = {\"max\": checkMax, \"min\": checkMin, \"cv\": cv, \"in\": isIn}\n",
    "\n",
    "def split_func(string):\n",
    "    \"\"\"\n",
    "    Take a string like 'requiredIf(\"arg_name\")'\n",
    "    return the function name and the argument:\n",
    "    (requiredIf, arg_name)\n",
    "    \"\"\"\n",
    "    ind = string.index(\"(\")\n",
    "    return string[:ind], string[ind+1:-1].strip('\"')\n",
    "\n",
    "\n",
    "def test_type(value, value_type):\n",
    "    if not value:\n",
    "        return None\n",
    "    if value_type == \"String\":\n",
    "        if str(value) == value:\n",
    "            return None\n",
    "        else:\n",
    "            return \"should be string\"\n",
    "    elif value_type == \"Number\":\n",
    "        try:\n",
    "            float(value)\n",
    "            return None\n",
    "        except ValueError:\n",
    "            return '\"{}\" should be a number'.format(str(value))\n",
    "    elif value_type == \"Integer\":\n",
    "        if isinstance(value, str):\n",
    "            if str(int(value)) == value:\n",
    "                return None\n",
    "            else:\n",
    "                return '\"{}\" should be an integer'.format(str(value))\n",
    "        else:\n",
    "            if int(value) == value:\n",
    "                return None\n",
    "            else:\n",
    "                return '\"{}\" should be an integer'.format(str(value))\n",
    "    else:\n",
    "        return None\n",
    "    #String, Number, Integer, List, Matrix, Dictionary, Text\n",
    "    \n",
    "\n",
    "\n",
    "def validate_df(df, dm):\n",
    "    # check column validity\n",
    "    cols = df.columns\n",
    "    invalid_cols = [col for col in cols if col not in dm.index]\n",
    "    for validation_name, validation in dm.iterrows():\n",
    "        value_type = validation['type']\n",
    "        if validation_name in df.columns:\n",
    "            output = df[validation_name].apply(test_type, args=(value_type,))\n",
    "            df[\"type_pass\" + \"_\" + validation_name + \"_\" + value_type] = output\n",
    "\n",
    "        val_list = validation['validations']\n",
    "        if not val_list or isinstance(val_list, float):\n",
    "            continue\n",
    "        for num, val in enumerate(val_list):\n",
    "            func_name, arg = split_func(val)\n",
    "            if arg == \"magic_table_column\":\n",
    "                continue\n",
    "            # first validate for presence\n",
    "            if func_name in presence_operations:\n",
    "                func = presence_operations[func_name]\n",
    "                #grade = func(validation_name, df, arg, dm)\n",
    "                grade = func(validation_name, arg, dm, df)\n",
    "                pass_col_name = \"presence_pass_\" + validation_name + \"_\" + func.__name__\n",
    "                df[pass_col_name] = grade\n",
    "    \n",
    "            # then validate for correct values\n",
    "            elif func_name in value_operations:\n",
    "                func = value_operations[func_name]\n",
    "                if validation_name in df.columns:\n",
    "                    grade = df.apply(func, args=(validation_name, arg, dm, df), axis=1)\n",
    "                    col_name = \"value_pass_\" + validation_name + \"_\" + func.__name__\n",
    "                    if col_name in df.columns:\n",
    "                        num_range = range(1, 10)\n",
    "                        for num in num_range:\n",
    "                            if (col_name + str(num)) in df.columns:\n",
    "                                continue\n",
    "                            else:\n",
    "                                col_name = col_name + str(num)\n",
    "                                break\n",
    "                    df[col_name] = grade.astype(object)\n",
    "    return df\n",
    "\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# check that values pass validation\n",
    "# validation checks to add:\n",
    "# sv (suggested vocab)\n",
    "# requiredOneInGroup\n",
    "# requiredUnlessSynthetic\n",
    "\n",
    "\n",
    "# re-do upload_magic to use contribution-level (??)\n",
    "\n",
    "# first, do validations on each table in the contribution\n",
    "# this will include removing unneeded data (RmKeys from old upload_magic)\n",
    "# this will also include checking everything against the data model (strings are strings, etc.)g\n",
    "\n",
    "\n",
    "# next, splat out each table into a file and wrap it up.  give it a sensible name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#current_df = current_con.tables['sites'].df  \n",
    "#current_dm = current_con.tables['sites'].data_model.dm['sites']\n",
    "\n",
    "#current_df = validate_df(current_df, current_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_validation_col_names(df):\n",
    "    value_cols = df.columns.str.match(\"^value_pass_\")\n",
    "    present_cols = df.columns.str.match(\"^presence_pass\")\n",
    "    type_cols = df.columns.str.match(\"^type_pass_\")\n",
    "\n",
    "    value_col_names = df.columns[value_cols]\n",
    "    present_col_names = df.columns[present_cols]\n",
    "    type_col_names = df.columns[type_cols]\n",
    "\n",
    "    validation_cols = np.where(value_cols, value_cols, present_cols)\n",
    "    validation_cols = np.where(validation_cols, validation_cols, type_cols)\n",
    "    validation_col_names = df.columns[validation_cols]\n",
    "    return value_col_names, present_col_names, type_col_names, validation_col_names\n",
    "\n",
    "#value_col_names, present_col_names, type_col_names, validation_col_names = get_validation_col_names(current_df)\n",
    "#present_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# incorrect data type problems\n",
    "#current_df[type_col_names].dropna(how='all', axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# missing column problems\n",
    "#current_df[present_col_names].dropna(how='all', axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# value problems:\n",
    "#current_df[value_col_names].dropna(how='all', axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "canopy_exercise": {
     "cell_type": "<None>"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_col_name(string):\n",
    "    prefixes = [\"presence_pass_\", \"value_pass_\", \"type_pass_\"]\n",
    "    end = string.rfind(\"_\")\n",
    "    for prefix in prefixes:\n",
    "        if string.startswith(prefix):\n",
    "            return prefix[:-6], string[len(prefix):end]\n",
    "    return string, string\n",
    "\n",
    "\n",
    "def check_row(row):\n",
    "    ind = row[row.notnull()].index\n",
    "    values = row[row.notnull()].values\n",
    "    # to transformation with extract_col_name here???\n",
    "    return dict(zip(ind, values))\n",
    "\n",
    "#def check_row(row):\n",
    "#    return True\n",
    "\n",
    "\n",
    "def print_failures(failing_items, verbose=False, outfile=None):\n",
    "    if outfile:\n",
    "        ofile = open(outfile, \"w\")\n",
    "        ofile.write(\"\\t\".join([\"name\", \"row_number\", \"problem_type\", \"problem_col\", \"error_message\"]))\n",
    "        ofile.write(\"\\n\")\n",
    "    else:\n",
    "        ofile = None\n",
    "    for ind, row in failing_items.iterrows():\n",
    "        issues = row[\"issues\"]\n",
    "        string = \"{:10}  |  row number: {}\".format(ind, str(row[\"num\"]))\n",
    "        first_string = \"\\t\".join([str(ind), str(row[\"num\"])])\n",
    "        if verbose:\n",
    "            print first_string\n",
    "        #if outfile:\n",
    "        #    ofile.write(\"{}\\n\".format(string))\n",
    "        for key, issue in issues.items():\n",
    "            issue_type, issue_col = extract_col_name(key)\n",
    "            string = \"{:10}  |  {:10}  |  {}\".format(issue_type, issue_col, issue)\n",
    "            string = \"\\t\".join([issue_type, issue_col, issue])\n",
    "            if verbose:\n",
    "                print string\n",
    "            if outfile:\n",
    "                ofile.write(first_string + \"\\t\" + string + \"\\n\")\n",
    "    if outfile:\n",
    "        ofile.close()\n",
    "\n",
    "def get_all_failures(df, value_cols, type_cols, verbose=False, outfile=None):\n",
    "    df[\"num\"] = range(len(df))\n",
    "    # get column names for value & type validations\n",
    "    names = value_cols.union(type_cols)\n",
    "    # drop all non validation columns\n",
    "    value_problems = df[names.union([\"num\"])]\n",
    "    failing_items = value_problems.dropna(how=\"all\", subset=names)\n",
    "    if not len(failing_items):\n",
    "        if verbose:\n",
    "            print \"No problems\"\n",
    "        return\n",
    "    failing_items = failing_items.dropna(how=\"all\", axis=1)\n",
    "    # get names of the failing items\n",
    "    bad_items = list(failing_items.index)\n",
    "    # get index numbers of the failing items\n",
    "    bad_indices = list(failing_items[\"num\"])\n",
    "    zip(bad_indices, bad_items)\n",
    "    #failing_items.drop(\"num\", axis=1, inplace=True)#.apply(check_row, axis=1).values\n",
    "    failing_items['issues'] = failing_items.drop(\"num\", axis=1).apply(check_row, axis=1).values\n",
    "    # maybe do a transformation in here so that you get \"lon\" instead of \"value_pass_lon_checkMax\"\n",
    "    print_failures(failing_items, verbose, outfile)\n",
    "    return failing_items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_bad_rows_and_cols(df, validation_names, verbose=False):\n",
    "    df[\"num\"] = range(len(df))\n",
    "    problems = df[validation_names.union([\"num\"])]\n",
    "    problems = problems.dropna(how='all', axis=0, subset=validation_names)\n",
    "    problems = problems.dropna(how='all', axis=1)\n",
    "    if not len(problems):\n",
    "        return None, None, None\n",
    "    bad_rows = zip(list(problems[\"num\"]), list(problems.index))\n",
    "    #\n",
    "    bad_cols = problems.columns\n",
    "    prefixes = [\"value_pass_\", \"type_pass_\"]\n",
    "    missing_prefix = \"presence_pass_\"\n",
    "    problem_cols = []\n",
    "    missing_cols = []\n",
    "    for col in bad_cols:\n",
    "        pre, stripped_col = extract_col_name(col)\n",
    "        for prefix in prefixes:\n",
    "            if col.startswith(prefix):\n",
    "                problem_cols.append(stripped_col)\n",
    "                continue\n",
    "        if col.startswith(missing_prefix):\n",
    "            missing_cols.append(stripped_col)\n",
    "    if verbose:\n",
    "        print \"these rows have problems:\", bad_rows\n",
    "        print \"!!!\"\n",
    "        print \"these columns contain bad values:\", problem_cols\n",
    "        print \"!!!\"\n",
    "        print \"these required columns are missing:\", missing_cols\n",
    "    return bad_rows, problem_cols, missing_cols\n",
    "    \n",
    "#a, b, c = get_bad_rows_and_cols(current_df, validation_col_names)\n",
    "#if a:\n",
    "#    print \"bad rows:\", a[:10]\n",
    "#    print \"problems:\", b[:10]\n",
    "#    print \"missing:\", c[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measurements\n",
      "--\n",
      "--\n",
      "ages\n",
      "--\n",
      "--\n",
      "sites\n",
      "--\n",
      "--\n",
      "locations\n",
      "--\n",
      "--\n",
      "samples\n",
      "--\n",
      "--\n",
      "criteria\n",
      "--\n",
      "--\n",
      "contribution\n",
      "--\n",
      "--\n",
      "specimens\n",
      "--\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "## run through and validate entire contribution\n",
    "\n",
    "#dir_path = os.path.join(os.getcwd(), '3_0', 'Megiddo')\n",
    "#con = Contribution(dir_path)\n",
    "the_con = con\n",
    "\n",
    "\n",
    "#for dtype in the_con.tables.keys()[2:4]:\n",
    "# doesn't work as validate_table function\n",
    "def validate_table(the_con, dtype, verbose=False):    \n",
    "    # grab dataframe\n",
    "    current_df = the_con.tables[dtype].df\n",
    "    # grab data model\n",
    "    current_dm = the_con.tables[dtype].data_model.dm[dtype]\n",
    "    # run all validations (will add columns to current_df)\n",
    "    current_df = validate_df(current_df, current_dm)\n",
    "    # get names of the added columns\n",
    "    value_col_names, present_col_names, type_col_names, validation_col_names = get_validation_col_names(current_df)\n",
    "    # print out failure messages\n",
    "    ofile = os.path.join(os.getcwd(), \"{}_errors.txt\".format(dtype))\n",
    "    failing_items = get_all_failures(current_df, value_col_names, type_col_names, verbose, outfile=ofile)\n",
    "    bad_rows, bad_cols, missing_cols = get_bad_rows_and_cols(current_df, validation_col_names, False)\n",
    "    # delete all validation rows\n",
    "    current_df.drop(validation_col_names, axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "def validate_contribution(the_con):\n",
    "    for dtype in the_con.tables.keys():#, 'criteria']:\n",
    "        print dtype\n",
    "        validate_table(the_con, dtype)\n",
    "        print '--'\n",
    "        print '--'\n",
    "\n",
    "\n",
    "validate_contribution(current_con)\n",
    "\n",
    "# work: sites\n",
    "# doesn't work: specimens, samples, locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Removing:  ['citation_label', 'compilation', 'calculation_type', 'average_n_lines', 'average_n_planes', 'specimen_grade', 'site_vgp_lat', 'site_vgp_lon', 'direction_type', 'specimen_Z', 'magic_instrument_codes', 'cooling_rate_corr', 'cooling_rate_mcd', 'anisotropy_atrm_alt', 'anisotropy_apar_perc', 'anisotropy_F', 'anisotropy_F_crit', 'specimen_scat', 'specimen_gmax', 'specimen_frac', 'site_vadm', 'site_lon', 'site_vdm', 'site_lat', 'measurement_chi', 'specimen_k_prime', 'specimen_k_prime_sse', 'external_database_names', 'external_database_ids', 'Further Notes', 'Typology', 'Notes (Year/Area/Locus/Level)', 'Site', 'Object Number']\n",
      "-I- file /Users/nebula/Python/PmagPy/3_0/Megiddo/locations.txt  successfully read in\n",
      "locations written to  /Users/nebula/Python/PmagPy/3_0/Megiddo/upload.txt\n",
      "-I- file /Users/nebula/Python/PmagPy/3_0/Megiddo/samples.txt  successfully read in\n",
      "samples written to  /Users/nebula/Python/PmagPy/3_0/Megiddo/upload.txt\n",
      "-I- file /Users/nebula/Python/PmagPy/3_0/Megiddo/specimens.txt  successfully read in\n",
      "specimens written to  /Users/nebula/Python/PmagPy/3_0/Megiddo/upload.txt\n",
      "-I- file /Users/nebula/Python/PmagPy/3_0/Megiddo/sites.txt  successfully read in\n",
      "sites written to  /Users/nebula/Python/PmagPy/3_0/Megiddo/upload.txt\n",
      "-I- file /Users/nebula/Python/PmagPy/3_0/Megiddo/ages.txt  successfully read in\n",
      "ages written to  /Users/nebula/Python/PmagPy/3_0/Megiddo/upload.txt\n",
      "-I- file /Users/nebula/Python/PmagPy/3_0/Megiddo/measurements.txt  successfully read in\n",
      "measurements written to  /Users/nebula/Python/PmagPy/3_0/Megiddo/upload.txt\n",
      "-I- file /Users/nebula/Python/PmagPy/3_0/Megiddo/criteria.txt  successfully read in\n",
      "criteria written to  /Users/nebula/Python/PmagPy/3_0/Megiddo/upload.txt\n",
      "-I- file /Users/nebula/Python/PmagPy/3_0/Megiddo/contribution.txt  successfully read in\n",
      "contribution written to  /Users/nebula/Python/PmagPy/3_0/Megiddo/upload.txt\n",
      "File: /Users/nebula/Python/PmagPy/3_0/Megiddo/images.txt\n",
      "bad_file is bad or non-existent - skipping \n",
      "Finished preparing upload file: /Users/nebula/Python/PmagPy/3_0/Megiddo/Tel-Hazor_Tel-Megiddo_25.Jul.2016.txt \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/Users/nebula/Python/PmagPy/3_0/Megiddo/Tel-Hazor_Tel-Megiddo_25.Jul.2016.txt',\n",
       " '',\n",
       " None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def upload_magic(concat=0, dir_path='.', data_model=None):\n",
    "    \"\"\"                                                                                                                              \n",
    "    Finds all magic files in a given directory, and compiles them into an upload.txt file which can be uploaded into the MagIC datab\\\n",
    "ase.                                                                                                                                 \n",
    "    returns a tuple of either: (False, error_message, errors) if there was a problem creating/validating the upload file             \n",
    "    or: (filename, '', None) if the upload was fully successful                                                                      \n",
    "    \"\"\"\n",
    "    SpecDone=[]\n",
    "    locations = []\n",
    "    concat = int(concat)\n",
    "    files_list = [\"locations.txt\", \"samples.txt\", \"specimens.txt\", \"sites.txt\", \"ages.txt\", \"measurements.txt\",\n",
    "                  \"criteria.txt\", \"contribution.txt\", \"images.txt\"]\n",
    "    file_names = [os.path.join(dir_path, f) for f in files_list]\n",
    "    # begin the upload process                                                                                                       \n",
    "    up = os.path.join(dir_path, \"upload.txt\")\n",
    "    if os.path.exists(up):\n",
    "        os.remove(up)\n",
    "    RmKeys = ['citation_label', 'compilation', 'calculation_type', 'average_n_lines', 'average_n_planes',\n",
    "              'specimen_grade', 'site_vgp_lat', 'site_vgp_lon', 'direction_type', 'specimen_Z',\n",
    "              'magic_instrument_codes', 'cooling_rate_corr', 'cooling_rate_mcd', 'anisotropy_atrm_alt',\n",
    "              'anisotropy_apar_perc', 'anisotropy_F', 'anisotropy_F_crit', 'specimen_scat',\n",
    "              'specimen_gmax','specimen_frac', 'site_vadm', 'site_lon', 'site_vdm', 'site_lat',\n",
    "              'measurement_chi', 'specimen_k_prime','specimen_k_prime_sse','external_database_names',\n",
    "              'external_database_ids', 'Further Notes', 'Typology', 'Notes (Year/Area/Locus/Level)',\n",
    "              'Site', 'Object Number']\n",
    "    print \"-I- Removing: \", RmKeys\n",
    "    CheckDec = ['_dec', '_lon', '_azimuth', 'dip_direction']\n",
    "    CheckSign = ['specimen_b_beta']\n",
    "    last = file_names[-1]\n",
    "    methods, first_file = [], 1\n",
    "    for File in file_names:\n",
    "    # read in the data                                                                                                               \n",
    "        Data, file_type=pmag.magic_read(File)\n",
    "        if file_type != \"bad_file\":\n",
    "            print \"-I- file\", File, \" successfully read in\"\n",
    "            if len(RmKeys) > 0:\n",
    "                for rec in Data:\n",
    "                    # remove unwanted keys                                                                                           \n",
    "                    for key in RmKeys:                                            \n",
    "                        if key in rec.keys():\n",
    "                            del rec[key] # get rid of unwanted keys                                                                  \n",
    "                    # make sure b_beta is positive                                                                                   \n",
    "                    ##if 'specimen_b_beta' in rec.keys() and rec['specimen_b_beta']!=\"\": # ignore blanks                               \n",
    "                    ##    if float(rec['specimen_b_beta'])< 0:\n",
    "                    ##        rec['specimen_b_beta']=str(-float(rec['specimen_b_beta']))  # make sure value is positive                \n",
    "                    ##        print '-I- adjusted to positive: ','specimen_b_beta',rec['specimen_b_beta']\n",
    "                    # make all declinations/azimuths/longitudes in range 0=>360.                                                     \n",
    "                    rec = pmag.adjust_all_to_360(rec)\n",
    "            if file_type == 'locations':\n",
    "                for rec in Data:\n",
    "                    locations.append(rec['location'])\n",
    "\n",
    "            ## LJ: should we still do this??\n",
    "            if file_type=='samples': # check to only upload top priority orientation record!                                      \n",
    "                NewSamps, Done = [], []\n",
    "                for rec in Data:\n",
    "                    if rec['sample'] not in Done:\n",
    "                        #orient,az_type=pmag.get_orient(Data,rec['sample'])\n",
    "                        #NewSamps.append(orient)\n",
    "                        Done.append(rec['sample'])\n",
    "            #    Data=NewSamps\n",
    "            #    print 'only highest priority orientation record from samples.txt read in '\n",
    "                \n",
    "            ## LJ: leave this for validations??\n",
    "            if file_type=='specimens': #  only specimens that have sample names                                                   \n",
    "                NewData,SpecDone=[],[]\n",
    "                for rec in Data:\n",
    "                    if rec['sample'] in Done:\n",
    "                        NewData.append(rec)\n",
    "                        SpecDone.append(rec['specimen'])\n",
    "                    else:\n",
    "                        print 'no valid sample record found for: '\n",
    "                        print rec\n",
    "                Data=NewData\n",
    "                #print 'only measurements that have specimen/sample info'  \n",
    "                \n",
    "            ## LJ: leave this for validations??\n",
    "            if file_type=='measurements': #  only measurements that have specimen names                                        \n",
    "                no_specs = []\n",
    "                NewData=[]\n",
    "                for rec in Data:\n",
    "                    if rec['specimen'] in SpecDone:\n",
    "                        NewData.append(rec)\n",
    "                    else:\n",
    "                        print 'no valid specimen record found for: '\n",
    "                        print rec    \n",
    "                        no_specs.append(rec)\n",
    "                #print set([record['er_specimen_name'] for record in no_specs])                                                      \n",
    "                Data = NewData\n",
    "    # write out the data                                                                                                             \n",
    "            if len(Data) > 0:\n",
    "                if first_file == 1:\n",
    "                    keystring = pmag.first_rec(up,Data[0],file_type)\n",
    "                    first_file = 0\n",
    "                else:\n",
    "                    keystring = pmag.first_up(up,Data[0],file_type)\n",
    "                for rec in Data:\n",
    "    # collect the method codes                                                                                                       \n",
    "                    if \"method_codes\" in rec.keys():\n",
    "                        meths = rec[\"method_codes\"].split(':')\n",
    "                        for meth in meths:\n",
    "                            if meth.strip() not in methods:\n",
    "                                if meth.strip() != \"LP-DIR-\":\n",
    "                                    methods.append(meth.strip())\n",
    "                    try:\n",
    "                        pmag.putout(up,keystring,rec)\n",
    "                    except IOError:\n",
    "                        print '-W- File input error: slowing down'\n",
    "                        time.sleep(1)\n",
    "                        pmag.putout(up, keystring, rec)\n",
    "                        \n",
    "    # write out the file separator                                                                                                   \n",
    "            f = open(up, 'a')\n",
    "            f.write('>>>>>>>>>>\\n')\n",
    "            f.close()\n",
    "            print file_type, 'written to ',up\n",
    "        else:\n",
    "            print 'File:', File\n",
    "            print file_type, 'is bad or non-existent - skipping '\n",
    "\n",
    "    # write out the methods table     \n",
    "    ## LJ: no more methods table....\n",
    "    #first_rec, MethRec=1,{}\n",
    "    #for meth in methods:\n",
    "    #    MethRec[\"method_code\"] = meth\n",
    "    #    if first_rec==1:\n",
    "    #        meth_keys = pmag.first_up(up, MethRec, \"magic_methods\")\n",
    "    #    first_rec=0\n",
    "    #    try:\n",
    "    #        pmag.putout(up,meth_keys,MethRec)\n",
    "    #    except IOError:\n",
    "    #        print '-W- File input error: slowing down'\n",
    "    #        time.sleep(1)\n",
    "    #        pmag.putout(up,meth_keys,MethRec)\n",
    "    if concat == 1:\n",
    "        f = open(up, 'a')\n",
    "        f.write('>>>>>>>>>>\\n')\n",
    "        f.close()\n",
    "     \n",
    "    ## LJ: replace this with new validate upload\n",
    "    #if os.path.isfile(up):\n",
    "    #    import validate_upload\n",
    "    #    validated = False\n",
    "    #    validated, errors = validate_upload.read_upload(up, data_model)\n",
    "\n",
    "    #else:\n",
    "    #    print \"no data found, upload file not created\"\n",
    "    #    return False, \"no data found, upload file not created\", None\n",
    "\n",
    "    #rename upload.txt according to location + timestamp                                                                             \n",
    "    format_string = \"%d.%b.%Y\"\n",
    "    if locations:\n",
    "        locs = locations[:3]\n",
    "        #location = locations[0].replace(' ', '_')\n",
    "        locs = [loc.replace(' ', '-') for loc in locs]\n",
    "        location = \"_\".join(locs)\n",
    "        new_up = location + '_' + time.strftime(format_string) + '.txt'\n",
    "    else:\n",
    "        new_up = 'unknown_location_' + time.strftime(format_string) + '.txt'\n",
    "\n",
    "    new_up = os.path.join(dir_path, new_up)\n",
    "    if os.path.isfile(new_up):\n",
    "        fname, extension = os.path.splitext(new_up)\n",
    "        for i in range(1, 100):\n",
    "            if os.path.isfile(fname + \"_\" + str(i) + extension):\n",
    "                continue\n",
    "            else:\n",
    "                new_up = fname + \"_\" + str(i) + extension\n",
    "                break\n",
    "    if not up:\n",
    "        print \"-W- Could not create an upload file\"\n",
    "        return\n",
    "    os.rename(up, new_up)\n",
    "    print \"Finished preparing upload file: {} \".format(new_up)\n",
    "    ## LJ: add back in validation stuff\n",
    "    #if not validated:\n",
    "    #    print \"-W- validation of upload file has failed.\\nPlease fix above errors and try again.\\nYou may run into problems if you t\\\n",
    "#ry to upload this file to the MagIC database\"\n",
    "    #    return False, \"file validation has failed.  You may run into problems if you try to upload this file.\", errors\n",
    "    return new_up, '', None\n",
    "\n",
    "\n",
    "dir_path = os.path.join(os.getcwd(), '3_0', 'Megiddo')\n",
    "dir_path = os.path.join(os.getcwd(), '3_0', 'Osler')\n",
    "upload_magic(dir_path=dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in an existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one</th>\n",
       "      <th>two</th>\n",
       "      <th>three</th>\n",
       "      <th>four</th>\n",
       "      <th>five</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   one  two  three  four  five\n",
       "0    3  NaN    1.0     9     2\n",
       "1    5  6.0    5.0     3     6\n",
       "2    9  1.0    NaN     4     5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep all of df1, add in any extra from df2\n",
    "df1 = pd.DataFrame(np.random.randint(1, 10, (3, 5)), columns=['one', 'two', 'three', 'four', 'five'])\n",
    "df1.iloc[0, 1] = np.nan\n",
    "df1.iloc[2, 2] = np.nan\n",
    "df2 = pd.DataFrame(np.random.randint(1, 10, (3, 5)), columns=['one', 'three', 'five', 'seven', 'nine'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one</th>\n",
       "      <th>three</th>\n",
       "      <th>five</th>\n",
       "      <th>seven</th>\n",
       "      <th>nine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   one  three  five  seven  nine\n",
       "0    8      3     8      1     2\n",
       "1    6      3     5      1     1\n",
       "2    4      3     8      2     2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one</th>\n",
       "      <th>two</th>\n",
       "      <th>three</th>\n",
       "      <th>four</th>\n",
       "      <th>five</th>\n",
       "      <th>nine</th>\n",
       "      <th>seven</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   one  two  three  four  five  nine  seven\n",
       "0    3  NaN    1.0     9     2     2      1\n",
       "1    5  6.0    5.0     3     6     1      1\n",
       "2    9  1.0    3.0     4     5     2      2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_df2_cols = df2.columns.difference(df1.columns)\n",
    "unique_df2 = df2[unique_df2_cols]\n",
    "\n",
    "# this adds in all the unique columns that weren't in df1\n",
    "concat_df = pd.concat([df1, unique_df2], axis=1)\n",
    "# fills in null values in df1 with values from df2\n",
    "concat_df.fillna(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
